{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression and gradient descent basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sympy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define variables and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x0,x1,x2 = sympy.var('x0,x1,x2')\n",
    "\n",
    "f = sympy.Matrix([x0**2+x1*x2, x0+x1+x2])\n",
    "g = sympy.Matrix([sympy.sin(x0), x1**3+x0*x1, x2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map function g on parameters of function f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "substitution_fg = {'x0':g[0], 'x1':g[1], 'x2':g[2]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Function compozition fog = f(g(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fog = f.subs(substitution_fg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct Jacobian matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⎡                      ⎛         2⎞     ⎛       2⎞⎤\n",
      "⎢x₁⋅x₂ + sin(2⋅x₀)  x₂⋅⎝x₀ + 3⋅x₁ ⎠  x₁⋅⎝x₀ + x₁ ⎠⎥\n",
      "⎢                                                 ⎥\n",
      "⎢                              2                  ⎥\n",
      "⎣  x₁ + cos(x₀)       x₀ + 3⋅x₁            1      ⎦\n"
     ]
    }
   ],
   "source": [
    "Jfog = fog.jacobian([x0,x1,x2])\n",
    "Jfog.simplify()\n",
    "sympy.pprint(Jfog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jacobian of each function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Jf = f.jacobian([x0,x1,x2])\n",
    "Jg = g.jacobian([x0,x1,x2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rule of chaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⎡                      ⎛         2⎞     ⎛       2⎞⎤\n",
      "⎢x₁⋅x₂ + sin(2⋅x₀)  x₂⋅⎝x₀ + 3⋅x₁ ⎠  x₁⋅⎝x₀ + x₁ ⎠⎥\n",
      "⎢                                                 ⎥\n",
      "⎢                              2                  ⎥\n",
      "⎣  x₁ + cos(x₀)       x₀ + 3⋅x₁            1      ⎦\n"
     ]
    }
   ],
   "source": [
    "Jfog2 = Jf.subs(substitution_fg)*Jg\n",
    "Jfog2.simplify()\n",
    "sympy.pprint(Jfog2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Hessian matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⎡2  0  0⎤\n",
      "⎢       ⎥\n",
      "⎢0  0  1⎥\n",
      "⎢       ⎥\n",
      "⎣0  1  0⎦\n"
     ]
    }
   ],
   "source": [
    "f0 = sympy.Matrix([f[0]])\n",
    "Jf0 = f0.jacobian([x0,x1,x2])\n",
    "\n",
    "H = Jf0.jacobian([x0,x1,x2])\n",
    "sympy.pprint(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Random2DGaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, average_precision_score\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Random2DGaussian():\n",
    "    np.random.seed(100)\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.min_x = 0\n",
    "        self.max_x = 10\n",
    "        self.min_y = 0\n",
    "        self.max_y = 10\n",
    "        \n",
    "        centar_x = np.random.random_sample()\n",
    "        centar_y = np.random.random_sample()\n",
    "        self.mean = np.array([centar_x, centar_y])\n",
    "        \n",
    "        eigval_x = (np.random.random_sample()*(self.max_x - self.min_x)/5)**2\n",
    "        eigval_y = (np.random.random_sample()*(self.max_y - self.min_y)/5)**2\n",
    "        \n",
    "        D = np.array([[eigval_x, 0], [0, eigval_y]])\n",
    "        R = np.array([[45, 0], [0, 45]])\n",
    "        \n",
    "        self.covariance_matrix = R.T * D * R\n",
    "    \n",
    "    def get_sample(self, n, show=False):\n",
    "        assert(n > 0)\n",
    "\n",
    "        if show:\n",
    "            print 'Mean:\\n', self.mean\n",
    "            print '\\nCovariance matrix:\\n', self.covariance_matrix\n",
    "        \n",
    "        x, y = np.random.multivariate_normal(self.mean, self.covariance_matrix, size=n).T\n",
    "        return np.column_stack((x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAEACAYAAABbMHZzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nLyKSQ0r+IiI5pOQvIpJDSv4iIjn0/wGJfA/YMQT3OgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6f39790e90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "G = Random2DGaussian()\n",
    "X = G.get_sample(100)\n",
    "plt.scatter(X[:,0], X[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Argumenti\n",
    "  X:  podatci, np.array Nx2\n",
    "  Y_: indeksi razreda, np.array Nx1\n",
    "\n",
    "Povratne vrijednosti\n",
    "  w, b: parametri logističke regresije\n",
    "'''\n",
    "def binlogreg_train(X,Y_):\n",
    "    b = 0\n",
    "    w = np.random.randn(2)\n",
    "    \n",
    "    # HYPERPARAMETERS\n",
    "    param_niter = 1000\n",
    "    param_delta = 0.1\n",
    "    \n",
    "    # gradijentni spust (param_niter iteracija)\n",
    "    for i in range(param_niter):\n",
    "        # klasifikacijski rezultati\n",
    "        scores = np.dot(X, w) + b\n",
    "        #print 'Scores', scores\n",
    "\n",
    "        # vjerojatnosti razreda c_1\n",
    "        probs = float(1) / (1 + np.exp(-scores))\n",
    "        #probs = np.exp(scores) / (1 + np.exp(scores))\n",
    "        #print 'probs', probs\n",
    "\n",
    "        # gubitak\n",
    "        loss  = np.sum(-np.log(probs))\n",
    "#         step1 = Y_ * np.log(probs)\n",
    "#         step2 = (1-Y_) * np.log(1 - probs)\n",
    "#         final = -step1 - step2\n",
    "#         loss = np.mean(final)\n",
    "        #print 'loss', loss\n",
    "\n",
    "        # dijagnosticki ispis\n",
    "        if i % 10 == 0:\n",
    "            print(\"iteration {}: loss {}\".format(i, loss))\n",
    "\n",
    "        # derivacije gubitka po klasifikacijskom rezultatu\n",
    "        dL_dscores = scores - probs\n",
    "        #print 'dL_dscores', dL_dscores\n",
    "\n",
    "        # gradijenti parametara\n",
    "        grad_w = np.dot(dL_dscores, X)\n",
    "        grad_b = np.sum(dL_dscores)\n",
    "        #print 'grad_w', grad_w\n",
    "        #print 'grad_b', grad_b\n",
    "\n",
    "        # poboljšani parametri\n",
    "        w += -param_delta * grad_w\n",
    "        b += -param_delta * grad_b\n",
    "\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "  Argumenti\n",
    "      X:    podatci, np.array Nx2\n",
    "      w, b: parametri logističke regresije\n",
    "\n",
    "  Povratne vrijednosti\n",
    "      probs: vjerojatnosti razreda c1\n",
    "'''\n",
    "def binlogreg_classify(X, w, b):\n",
    "    pred_prob = np.dot(w, X.T) + b\n",
    "    probs = np.where(pred_prob >= .5, 1, 0)\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_gauss_2d(C, N):\n",
    "    G = Random2DGaussian()\n",
    "    Y_ = np.random.choice([0, 1], size=(N,), p=[1./2, 1./2])\n",
    "    return G.get_sample(N), Y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def eval_perf_binary(Y, Y_):\n",
    "    return accuracy_score(Y, Y_), recall_score(Y, Y_), precision_score(Y, Y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "def eval_AP(Y_sorted):\n",
    "    return 0.5\n",
    "    \n",
    "print eval_AP([0,0,0,1,1,1])\n",
    "#1.0\n",
    "print eval_AP([0,0,1,0,1,1])\n",
    "#0.9166666666666666\n",
    "print eval_AP([0,1,0,1,0,1])\n",
    "#0.7555555555555555\n",
    "print eval_AP([1,0,1,0,1,0])\n",
    "#0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0: loss 3249.63266934\n",
      "iteration 10: loss inf\n",
      "iteration 20: loss inf\n",
      "iteration 30: loss inf\n",
      "iteration 40: loss inf\n",
      "iteration 50: loss inf\n",
      "iteration 60: loss inf\n",
      "iteration 70: loss nan\n",
      "iteration 80: loss nan\n",
      "iteration 90: loss nan\n",
      "iteration 100: loss nan\n",
      "iteration 110: loss nan\n",
      "iteration 120: loss nan\n",
      "iteration 130: loss nan\n",
      "iteration 140: loss nan\n",
      "iteration 150: loss nan\n",
      "iteration 160: loss nan\n",
      "iteration 170: loss nan\n",
      "iteration 180: loss nan\n",
      "iteration 190: loss nan\n",
      "iteration 200: loss nan\n",
      "iteration 210: loss nan\n",
      "iteration 220: loss nan\n",
      "iteration 230: loss nan\n",
      "iteration 240: loss nan\n",
      "iteration 250: loss nan\n",
      "iteration 260: loss nan\n",
      "iteration 270: loss nan\n",
      "iteration 280: loss nan\n",
      "iteration 290: loss nan\n",
      "iteration 300: loss nan\n",
      "iteration 310: loss nan\n",
      "iteration 320: loss nan\n",
      "iteration 330: loss nan\n",
      "iteration 340: loss nan\n",
      "iteration 350: loss nan\n",
      "iteration 360: loss nan\n",
      "iteration 370: loss nan\n",
      "iteration 380: loss nan\n",
      "iteration 390: loss nan\n",
      "iteration 400: loss nan\n",
      "iteration 410: loss nan\n",
      "iteration 420: loss nan\n",
      "iteration 430: loss nan\n",
      "iteration 440: loss nan\n",
      "iteration 450: loss nan\n",
      "iteration 460: loss nan\n",
      "iteration 470: loss nan\n",
      "iteration 480: loss nan\n",
      "iteration 490: loss nan\n",
      "iteration 500: loss nan\n",
      "iteration 510: loss nan\n",
      "iteration 520: loss nan\n",
      "iteration 530: loss nan\n",
      "iteration 540: loss nan\n",
      "iteration 550: loss nan\n",
      "iteration 560: loss nan\n",
      "iteration 570: loss nan\n",
      "iteration 580: loss nan\n",
      "iteration 590: loss nan\n",
      "iteration 600: loss nan\n",
      "iteration 610: loss nan\n",
      "iteration 620: loss nan\n",
      "iteration 630: loss nan\n",
      "iteration 640: loss nan\n",
      "iteration 650: loss nan\n",
      "iteration 660: loss nan\n",
      "iteration 670: loss nan\n",
      "iteration 680: loss nan\n",
      "iteration 690: loss nan\n",
      "iteration 700: loss nan\n",
      "iteration 710: loss nan\n",
      "iteration 720: loss nan\n",
      "iteration 730: loss nan\n",
      "iteration 740: loss nan\n",
      "iteration 750: loss nan\n",
      "iteration 760: loss nan\n",
      "iteration 770: loss nan\n",
      "iteration 780: loss nan\n",
      "iteration 790: loss nan\n",
      "iteration 800: loss nan\n",
      "iteration 810: loss nan\n",
      "iteration 820: loss nan\n",
      "iteration 830: loss nan\n",
      "iteration 840: loss nan\n",
      "iteration 850: loss nan\n",
      "iteration 860: loss nan\n",
      "iteration 870: loss nan\n",
      "iteration 880: loss nan\n",
      "iteration 890: loss nan\n",
      "iteration 900: loss nan\n",
      "iteration 910: loss nan\n",
      "iteration 920: loss nan\n",
      "iteration 930: loss nan\n",
      "iteration 940: loss nan\n",
      "iteration 950: loss nan\n",
      "iteration 960: loss nan\n",
      "iteration 970: loss nan\n",
      "iteration 980: loss nan\n",
      "iteration 990: loss nan\n",
      "(0.52000000000000002, 0.0, 0.0, 0.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/weenkus/anaconda3/envs/py27/lib/python2.7/site-packages/ipykernel/__main__.py:24: RuntimeWarning: overflow encountered in exp\n",
      "/home/weenkus/anaconda3/envs/py27/lib/python2.7/site-packages/ipykernel/__main__.py:29: RuntimeWarning: divide by zero encountered in log\n",
      "/home/weenkus/anaconda3/envs/py27/lib/python2.7/site-packages/ipykernel/__main__.py:11: RuntimeWarning: invalid value encountered in greater_equal\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(100)\n",
    "\n",
    "# get the training dataset\n",
    "X,Y_ = sample_gauss_2d(2, 100)\n",
    "\n",
    "# train the model\n",
    "w,b = binlogreg_train(X, Y_)\n",
    "\n",
    "# evaluate the model on the training dataset\n",
    "probs = binlogreg_classify(X, w,b)\n",
    "Y = np.where(probs >= .5, 1, 0)\n",
    "\n",
    "# report performance\n",
    "accuracy, recall, precision = eval_perf_binary(Y, Y_)\n",
    "AP = eval_AP(Y_[probs.argsort()])\n",
    "print (accuracy, recall, precision, AP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named Tensorflow",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-168-d6f8a66bb597>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mTensorflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: No module named Tensorflow"
     ]
    }
   ],
   "source": [
    "import Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
